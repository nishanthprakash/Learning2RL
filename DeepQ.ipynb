{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "class two_step_mdp():\n",
    "    def __init__(self, tranprob = 0.75, rewprob = 0.9):\n",
    "        self.state_space = (0,)\n",
    "        self.action_space = (0, 1)\n",
    "        self.COMMON = True\n",
    "        self.RARE = False\n",
    "        self.state = 0\n",
    "        self.tran_prob = tranprob\n",
    "        self.rew_prob = rewprob\n",
    "        self.transition = {0: lambda: (1, self.COMMON) if np.random.rand(1) < self.tran_prob else (2, self.RARE),\n",
    "                    1: lambda: (2, self.COMMON) if np.random.rand(1) < self.tran_prob else (1, self.RARE)}\n",
    "        self.reward = {1: lambda: 1 if np.random.rand(1) < self.rew_prob else 0,\n",
    "                    2: lambda: 1 if np.random.rand(1) < 1 - self.rew_prob else 0}\n",
    "        self.original = True\n",
    "        #self.switch()\n",
    "        \n",
    "    def get_state(self):\n",
    "        return self.state\n",
    "    \n",
    "    def get_reward(self):\n",
    "        return self.reward[self.state]()\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.state, common = self.transition[action]()\n",
    "        return self.state, self.reward[self.state](), common\n",
    "    \n",
    "    def rand_step(self):\n",
    "        #self.state = self.transition[((np.random.random(1)<0.5)[0])]()\n",
    "        #return self.state, self.reward[self.state]()\n",
    "        return (np.random.random(1)<0.5)[0]\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = 0\n",
    "        return self.state\n",
    "    \n",
    "    def switch(self):\n",
    "        self.rew_prob = (1-self.rew_prob) if (np.random.random(1)<0.5)[0] else self.rew_prob\n",
    "        self.original = not self.original\n",
    "        return self.original\n",
    "\n",
    "\n",
    "num_trials = 10\n",
    "num_episodes = 20\n",
    "\n",
    "inptSt = tf.placeholder(dtype=tf.int32)\n",
    "oneH = tf.one_hot(inptSt,3)\n",
    "Q = tf.Variable(tf.random_uniform([3,2],0,0.01))\n",
    "qVals= tf.matmul([oneH],Q)\n",
    "outAct= tf.argmax(qVals,1)\n",
    "\n",
    "nextQ = tf.placeholder(shape=[1,2],dtype=tf.float32)\n",
    "loss = tf.reduce_sum(tf.square(nextQ - qVals))\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "updateMod = trainer.minimize(loss)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "gamma = 1\n",
    "game = two_step_mdp()\n",
    "rTot=0\n",
    "stayNumCommReward=0\n",
    "stayNumCommNoReward=0\n",
    "stayNumRareReward=0\n",
    "stayNumRareNoReward=0\n",
    "numCommRewarded=0\n",
    "numCommNotRewarded=0\n",
    "numRareRewarded=0\n",
    "numRareNotRewarded=0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(num_episodes):\n",
    "        game.switch()\n",
    "        for j in range(num_trials):\n",
    "            e = 0.05/(i + 0.05)\n",
    "            s=game.reset()\n",
    "            nActs, nxtQ=sess.run([outAct,qVals],feed_dict={inptSt: s})\n",
    "            if i>=1:\n",
    "                if rwd:      \n",
    "                    if comm:\n",
    "                        stayNumCommReward += 1 if nActs[0] == nAct else 0\n",
    "                        numCommRewarded += 1\n",
    "                    else:\n",
    "                        stayNumRareReward += 1 if nActs[0] == nAct else 0\n",
    "                        numRareRewarded += 1\n",
    "                else:\n",
    "                    if comm:\n",
    "                        stayNumCommNoReward += 1 if nActs[0] == nAct else 0\n",
    "                        numCommNotRewarded += 1\n",
    "                    else:\n",
    "                        stayNumRareNoReward += 1 if nActs[0] == nAct else 0\n",
    "                        numRareNotRewarded += 1\n",
    "            print(nActs[0]==nAct)\n",
    "            nAct=nActs[0]\n",
    "            if i<20 and np.random.rand(1)<e: nAct= game.rand_step()\n",
    "            s1, rwd, comm = game.step(nAct)\n",
    "            Q1 = sess.run(qVals,feed_dict={inptSt: s1})\n",
    "            print(nxtQ)\n",
    "            nxtQ[0,nAct] = rwd + gamma*(np.max(Q1))\n",
    "            sess.run(updateMod,feed_dict={inptSt:s, nextQ:nxtQ})\n",
    "            rTot+=rwd\n",
    "        print(\"-----------\")\n",
    "\n",
    "#print \"Fraction of games succesful: \", rTot/2000.0\n",
    "\n",
    "print stayNumCommReward/(1.0*numCommRewarded)\n",
    "print stayNumRareReward/(1.0*numRareRewarded)\n",
    "print stayNumCommNoReward/(1.0*numCommNotRewarded)\n",
    "print stayNumRareNoReward/(1.0*numRareNotRewarded)\n",
    "\n",
    "print stayNumCommReward\n",
    "print stayNumRareReward\n",
    "print stayNumCommNoReward\n",
    "print stayNumRareNoReward\n",
    "\n",
    "print numCommRewarded\n",
    "print numRareRewarded\n",
    "print numCommNotRewarded\n",
    "print numRareNotRewarded"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
